# Awesome-Multimodal-LLM-for-Math/STEM [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

![](assets/logo.webp)

üî• Collections of multi-modal LLM for Math/STEM/Code.

## Table of Content

- [Awesome-Multimodal-LLM-for-Math/STEM ](#awesome-multimodal-llm-for-mathstem-)
  - [Table of Content](#table-of-content)
  - [Awesome Papers](#awesome-papers)
  - [MLLM Math/STEM Dataset](#mllm-mathstem-dataset)
  - [MLLM Math/STEM Benchmark](#mllm-mathstem-benchmark)
  - [Contributors](#contributors)

## Awesome Papers
1. **MAVIS: Mathematical Visual Instruction Tuning** `Preprint`
   
   _Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo,Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, Peng Gao, Hongsheng Li._[[Paper](https://arxiv.org/pdf/2407.08739)], 2024.7

1. **COMET: ‚ÄúCone of experience‚Äù enhanced large multimodal model for mathematical problem generation.** `Preprint`

   _Sannyuya Liu, Jintian Feng, Zongkai Yang, Yawei Luo, Qian Wan, Xiaoxuan Shen, Jianwen Sun._ [[Paper](https://arxiv.org/abs/2407.11315v1)], 2024.7

1. **Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B: A Technical Report.** `Preprint`

   _Di Zhang, Xiaoshui Huang, Dongzhan Zhou, Yuqiang Li, Wanli Ouyang._ [[Paper](https://arxiv.org/abs/2406.07394)], 2024.6

1. **Visual SKETCHPAD: Sketching as a Visual Chain of Thought for Multimodal Language Models.** `Preprint`

   _Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, Ranjay Krishna._ [[Paper](https://arxiv.org/abs/2406.09403)], [[Code](https://visualsketchpad.github.io/)], 2024.6

1. **TextSquare: Scaling up Text-Centric Visual Instruction Tuning.** `Preprint`

   _Jingqun Tang, Chunhui Lin, Zhen Zhao, Shu Wei, Binghong Wu, Qi Liu, Hao Feng, Yang Li, Siqi Wang, Lei Liao, Wei Shi, Yuliang Liu, Hao Liu, Yuan Xie, Xiang Bai, Can Huang._ [[Paper](https://arxiv.org/abs/2404.12803)], 2024.4

1. **Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs.** `ACL 2024`

   _Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, Jingren Zhou._ [[Paper](https://arxiv.org/abs/2403.12596)], 2024.3

1. **mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding.** `Preprint`

   _Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, Jingren Zhou._ [[Paper](https://arxiv.org/abs/2403.12895)], 2024.3

1. **ChartX & ChartVLM: A Versatile Benchmark and Foundation Model for Complicated Chart Reasoning.** `Preprint`

   _Renqiu Xia, Bo Zhang, Hancheng Ye, Xiangchao Yan, Qi Liu, Hongbin Zhou, Zijun Chen, Min Dou, Botian Shi, Junchi Yan, Yu Qiao._ [[Paper](https://arxiv.org/abs/2402.12185)], 2024.2

1. **InstructDoc: A Dataset for Zero-Shot Generalization of Visual Document Understanding with Instructions.** `Preprint`

   _Ryota Tanaka, Taichi Iki, Kyosuke Nishida, Kuniko Saito, Jun Suzuki._ [[Paper](https://arxiv.org/abs/2401.13313)], 2024.1

1. **G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model.** `Preprint`

    _Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, Lingpeng Kong._ [[Paper](https://arxiv.org/abs/2312.11370)], [[Code](https://github.com/pipilurj/G-LLaVA)], 2023.12

1. **mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Models.** `Preprint`

   _Anwen Hu, Yaya Shi, Haiyang Xu, Jiabo Ye, Qinghao Ye, Ming Yan, Chenliang Li, Qi Qian, Ji Zhang, Fei Huang._ [[Paper](https://arxiv.org/abs/2311.18248)], 2023.11

## MLLM Math/STEM Dataset

| Name                                                                                       |                                                                                                                                                  Paper                                                                                                                                                   |                                                                                                                           Notes                                                                                                                           |
| :----------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
| [**ScienceQA**](https://scienceqa.github.io)                                               |                                                                                       [Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering](https://arxiv.org/abs/2209.09513)                                                                                       |                                                                              A benchmark consists of ‚àº21k multimodal multiple choice questions with diverse science topics.                                                                               |
| [**CMM12K**]()                                                                             |                                                                                  [COMET: ‚ÄúCone of experience‚Äù enhanced large multimodal model for mathematical problem generation](https://arxiv.org/abs/2407.11315v1)                                                                                   |                                                                                                      A Chinese MM SFT dataset for math, not released                                                                                                      |
| [**SPIQA**](https://huggingface.co/datasets/google/spiqa)                                  |                                                                                               [SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers](https://arxiv.org/abs/2407.09413)                                                                                                |                                                      Designed to interpret complex figures and tables within the context of scientific research articles across various domains of computer science                                                       |
| [**InstructDoc**](https://github.com/nttmdlab-nlp/InstructDoc)                             |                                                                                [InstructDoc: A Dataset for Zero-Shot Generalization of Visual Document Understanding with Instructions](https://arxiv.org/abs/2401.13313)                                                                                |                                                                         Collection of 30 publicly avail- able VDU datasets, each with diverse instructions in a uni- fied format.                                                                         |
| [**M-Paper**](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/PaperOwl)                   |                                                                                         [mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Model](https://arxiv.org/abs/2311.18248)                                                                                         |                                                                                                Built by parsing Latex source files of high-quality papers.                                                                                                |
| [**DocStruct4M/DocReason25K**](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5) |                                                                                           [mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding](https://arxiv.org/abs/2403.12895)                                                                                           |                                                                                    Based on publicly available datasets. / A high-quality instruction tuning dataset.                                                                                     |
| [**DocGenome**](https://unimodal4reasoning.github.io/DocGenome_page)                       |                                                                       [DocGenome: An Open Large-scale Scientific Document Benchmark for Training and Testing Multi-modal Large Language Models](https://arxiv.org/abs/2406.11633)                                                                        |                                                                  A structured document benchmark constructed by annotating 500K scientific documents from 153 disciplines in the arXiv.                                                                   |
| [**ArXivCap/ArXivQA**](https://mm-arxiv.github.io)                                         |                                                                                  [Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models](https://arxiv.org/abs/2403.00231)                                                                                  |                             A figure-caption dataset comprising 6.4M images and 3.9M cap- tions, sourced from 572K ArXiv papers. / a question- answering dataset generated by prompting GPT- 4V based on scientific figures.                              |
| [**FigureQA**](https://github.com/Maluuba/FigureQA)                                        |                                                                                                      [FigureQA: An Annotated Figure Dataset for Visual Reasoning](https://arxiv.org/abs/1710.07300)                                                                                                      | A visual reasoning corpus of over one million question-answer pairs grounded in over 100,000 images. The images are synthetic, scientific-style figures from five classes: line plots, dot-line plots, vertical and horizontal bar graphs, and pie charts |
| [**DVQA**](https://github.com/kushalkafle/DVQA_dataset)                                    |                                                                                                    [DVQA: Understanding Data Visualizations via Question Answering](https://arxiv.org/abs/1801.08163)                                                                                                    |                                                                              A dataset that tests many aspects of bar chart understanding in a question answering framework.                                                                              |
| [**SciGraphQA**](https://github.com/findalexli/SciGraphQA)                                 |                                                                                   [SciGraphQA: A Large-Scale Synthetic Multi-Turn Question-Answering Dataset for Scientific Graphs](https://arxiv.org/abs/2308.03349)                                                                                    |                                                                                        a synthetic multi-turn question-answer dataset related to academic graphs.                                                                                         |
| [**SciCap**](https://github.com/tingyaohsu/SciCap)                                         |                                                                                                 [SciCap: Generating Captions for Scientific Figures](https://aclanthology.org/2021.findings-emnlp.277/)                                                                                                  |                                            A large-scale figure caption dataset based on Computer Science arXiv papers published between 2010 and 2020, contained over 416k figures that focused on graphplot.                                            |
| [**FigCap**]()                                                                             |                                                                                                     [Figure Captioning with Reasoning and Sequence-Level Training](https://arxiv.org/pdf/1906.02850)                                                                                                     |                                                                                                                Generated based on FigureQA                                                                                                                |
| [**FigureSeer**](https://prior.allenai.org/projects/figureseer)                            |                                                                                       [FigureSeer: Parsing Result-Figures in Research Papers](https://ai2-website.s3.amazonaws.com/publications/Siegel16eccv.pdf)                                                                                        |                                                                                                                             -                                                                                                                             |
| [**UniChart**](https://github.com/vis-nlp/UniChart)                                        |                                                                                     [UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning](https://arxiv.org/abs/2305.14761)                                                                                     |                                                                             A large-scale chart corpus for pretraining, covering a diverse range of visual styles and topics.                                                                             |
| [**MapQA**](https://github.com/OSU-slatelab/MapQA)                                         |                                                                                                      [MapQA: A Dataset for Question Answering on Choropleth Maps](https://arxiv.org/abs/2211.08545)                                                                                                      |                                                                                        A large-scale dataset of ~800K question-answer pairs over ~60K map images.                                                                                         |
| [**TabMWP**](https://promptpg.github.io/index.html#dataset)                                |                                                                                        [Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning](https://arxiv.org/abs/2209.14610)                                                                                        |                                                             A dataset containing 38,431 open-domain grade-level problems that require mathematical reasoning on both textual and tabular data                                                             |
| [**CLEVR-Math**](https://github.com/dali-does/clevr-math)                                  |                                                                                         [CLEVR-Math: A Dataset for Compositional Language, Visual and Mathematical Reasoning](https://arxiv.org/abs/2208.05358)                                                                                          |                                                                      A multi-modal math word problems dataset consisting of simple math word problems involving addition/subtraction                                                                      |
| [**GUICourse**](https://github.com/yiye3/GUICourse)                                        |                                                                                                 [GUICourse: From General Vision Language Model to Versatile GUI Agent](https://arxiv.org/abs/2406.11317)                                                                                                 |                                                                                          A suite of datasets to train visual-based GUI agents from general VLMs                                                                                           |
| [**PIN-14M**](https://huggingface.co/datasets/m-a-p/PIN-14M)                               |                                                                                          [PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents](https://arxiv.org/abs/2406.13923)                                                                                          |                                                                   14 million samples derived from Chinese and English sources, tailored to include complex web and scientific content.                                                                    |
| [**Math V360K**](https://github.com/HZQ950419/Math-LLaVA)                                  |                                                                                        [Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models](https://arxiv.org/abs/2406.17294)                                                                                         |                                                                       40K high-quality images with question-answer pairs from 24 existing datasets and synthesizing 320K new pairs.                                                                       |
| [**MMSci**](https://github.com/Leezekun/MMSci)                                             |                                                                        [MMSci: A Multimodal Multi-Discipline Dataset for PhD-Level Scientific Comprehension](https://arxiv.org/abs/2407.04903?fbclid=IwZXh0bgNhZW0CMTEAAR0oph0y)                                                                         |                                                                     Collected a multimodal dataset from open-access scientific articles published in Nature Communications journals.                                                                      |
| [**MAVIS-Caption/Instruct**](https://github.com/ZrrSkywalker/MAVIS)                        |                                                                                                            [MAVIS: Mathematical Visual Instruction Tuning](https://arxiv.org/abs/2407.08739)                                                                                                             |                                                                                                                             -                                                                                                                             |
| [**Geo170K**](https://github.com/pipilurj/G-LLaVA)                                         |                                                                                               [G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model](https://arxiv.org/abs/2312.11370)                                                                                               |                                                                     Utilize the geometry characteristic to construct a multi-modal geometry dataset, building upon existing datasets.                                                                     |
| [**SciOL/MuLMS-Img**](https://github.com/boschresearch/)                                   | [SciOL and MuLMS-Img: Introducing A Large-Scale Multimodal Scientific Dataset and Models for Image-Text Tasks in the Scientific Domain](https://openaccess.thecvf.com/content/WACV2024/papers/Tarsi_SciOL_and_MuLMS-Img_Introducing_a_Large-Scale_Multimodal_Scientific_Dataset_and_WACV_2024_paper.pdf) |                                                                                            Pretraining corpus for multimodal models in the scientific domain.                                                                                             |
| [**PlotQA**](https://github.com/NiteshMethani/PlotQA/blob/master/PlotQA_Dataset.md)        |                                                                                                               [PlotQA: Reasoning over Scientific Plots](https://arxiv.org/abs/1909.00997)                                                                                                                |                                                    With 28.9 million question-answer pairs over 224,377 plots on data from realworld sources and questions based on crowd-sourced question templates.                                                     |

## MLLM Math/STEM Benchmark

| Name                                                                                |                                                                                   Paper                                                                                   |                                                                        Note                                                                        |
| :---------------------------------------------------------------------------------- | :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :------------------------------------------------------------------------------------------------------------------------------------------------: |
| [**GeoEval**](https://github.com/GeoEval/GeoEval)                                   |                       [GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving](https://arxiv.org/abs/2402.10104)                       |                                  An benchmark for evaluating MLLMs' capability in solving geometry math problems                                   |
| [**Geometry3K**](https://lupantech.github.io/inter-gps)                             |                    [Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning](https://arxiv.org/pdf/2105.04165v3)                    |                                  Consisting of 3,002 geometry problems with dense annotation in formal language.                                   |
| [**GEOS**](https://github.com/seominjoon/geosolver)                                 |                               [Solving Geometry Problems: Combining Text and Diagram Interpretation](https://aclanthology.org/D15-1171.pdf)                               |                                                                         -                                                                          |
| [**GeoQA**](https://github.com/chen-judge/GeoQA)                                    |                        [GeoQA: A Geometric Question Answering Benchmark Towards Multimodal Numerical Reasoning](https://arxiv.org/abs/2105.14517)                         |                                          4,998 geometric problems with cor- responding annotated programs                                          |
| [**GeoQA+**](https://arxiv.org/abs/2212.02746)                                      |            [An Augmented Benchmark Dataset for Geometric Question Answering through Dual Parallel Text Encoding](https://aclanthology.org/2022.coling-1.130/)             |                          Based on GeoQA, newly annotate 2,518 geometric problems with richer types and greater difficulty                          |
| [**UniGeo**](https://github.com/chen-judge/UniGeo)                                  |                         [UniGeo: Unifying Geometry Logical Reasoning via Reformulating Mathematical Expression](https://arxiv.org/abs/2212.02746)                         |                                           Contains 4,998 calculation problems and 9,543 proving problems                                           |
| [**PGPS9K**](https://nlpr.ia.ac.cn/databases/CASIA-PGPS9K/index.html)               |                           [A Multi-Modal Neural Geometric Solver with Textual Clauses Parsed from Diagram](https://arxiv.org/pdf/2302.11097v2)                            |                               Labeled with both fine-grained diagram annotation and interpretable solution program.                                |
| [**GeomVerse**](https://storage.googleapis.com/gresearch/GeomVerseV0/GeomVerse.zip) |                              [GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning](https://arxiv.org/abs/2312.12241)                               |                        A synthetic benchmark of geometry questions with controllable difficulty levels along multiple axes                         |
| [**MathVista**](https://mathvista.github.io)                                        |                         [MATHVISTA: EVALUATING MATHEMATICAL REASONING OF FOUNDATION MODELS IN VISUAL CONTEXTS](https://arxiv.org/abs/2310.02255)                          |                               A benchmark designed to combine challenges from diverse mathematical and visual tasks.                               |
| [**OlympiadBench**](https://github.com/OpenBMB/OlympiadBench)                       |         [OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems](https://arxiv.org/abs/2402.14008)         |                       An Olympiad-level bilingual multimodal scientific benchmark, from mathematics and physics competitions                       |
| [**OlympicArena**](https://gair-nlp.github.io/OlympicArena/)                        |                        [OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI](https://arxiv.org/abs/2406.12753)                        |                       Encompass a wide range of disciplines spanning seven fields and 62 international Olympic competitions.                       |
| [**SciBench**](https://github.com/mandyyyyii/scibench)                              |                  [SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models](https://arxiv.org/pdf/2307.10635v3)                   |                              A benchmark for college-level scientific problems sourced from instructional textbooks.                               |
| [**MMMU**](https://mmmu-benchmark.github.io/)                                       |                   [MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI](https://arxiv.org/abs/2311.16502)                    |    Designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning.    |
| [**MathVerse**](https://mathverse-cuhk.github.io)                                   |                         [MATHVERSE: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?](https://arxiv.org/abs/2403.14624)                          |                           2,612 high-quality, multi-subject math problems with diagrams from publicly available sources.                           |
| [**MATH-Vision**](https://mathvision-cuhk.github.io)                                |                                 [Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset](https://arxiv.org/abs/2402.14804)                                  |                        3,040 high-quality mathe- matical problems with visual contexts sourced from real math competitions.                        |
| [**AI2D**](https://github.com/allenai/dqa-net)                                      |                                                  [A Diagram Is Worth A Dozen Images](https://arxiv.org/pdf/1603.07396v1)                                                  |                  A dataset of diagrams with annotations of constituents and relationships for over 5,000 diagrams and 15,000 QAs.                  |
| [**IconQA**](https://iconqa.github.io/)                                             |                       [IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning](https://arxiv.org/abs/2110.13214)                        |                                    A benchmark with the goal of answering a question in an icon image context.                                     |
| [**TQA**](https://allenai.org/data/tqa)                                             | [Are You Smarter Than A Sixth Grader? Textbook Question Answering for Multimodal Machine Comprehension](https://ai2-website.s3.amazonaws.com/publications/CVPR17_TQA.pdf) |                        Includes 1,076 lessons and 26,260 multi-modal questions, taken from middle school science curricula.                        |
| [**ScienceQA**](https://scienceqa.github.io)                                        |                       [Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering](https://arxiv.org/abs/2209.09513)                        |                           A benchmark consists of ‚àº21k multimodal multiple choice questions with diverse science topics.                           |
| [**ChartX**](https://github.com/UniModal4Reasoning/ChartVLM)                        |                     [ChartX & ChartVLM: A Versatile Benchmark and Foundation Model for Complicated Chart Reasoning](https://arxiv.org/abs/2402.12185)                     |              A multi-modal evaluation set covering 18 chart types, 7 chart tasks, 22 disciplinary topics, and high-quality chart data              |
| [**PlotQA**](https://github.com/NiteshMethani/PlotQA/blob/master/PlotQA_Dataset.md) |                                                [PlotQA: Reasoning over Scientific Plots](https://arxiv.org/abs/1909.00997)                                                | With 28.9 million question-answer pairs over 224,377 plots on data from realworld sources and questions based on crowd-sourced question templates. |
| [**Chart-to-text**](https://github.com/vis-nlp/Chart-to-text)                       |                               [Chart-to-Text: A Large-Scale Benchmark for Chart Summarization](https://aclanthology.org/2022.acl-long.277/)                               |              A large-scale benchmark with two datasets and a total of 44,096 charts covering a wide range of topics and chart types.               |
| [**ChartQA**](https://github.com/vis-nlp/ChartQA)                                   |                      [ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning](https://arxiv.org/abs/2203.10244)                       |       A large-scale benchmark covering 9.6K human-written questions as well as 23.1K questions generated from human-written chart summaries.       |
| [**OpenCQA**](https://github.com/vis-nlp/OpenCQA)                                   |                                          [OpenCQA: Open-ended Question Answering with Charts](https://arxiv.org/abs/2210.06628)                                           |                                 The goal is to answer an open-ended question about a chart with descriptive texts.                                 |
| [**ChartBench**](https://chartbench.github.io/)                                     |                                    [ChartBench: A Benchmark for Complex Visual Reasoning in Charts](https://arxiv.org/abs/2312.15915)                                     |              A comprehensive benchmark designed to assess chart comprehension and data reliability through complex visual reasoning.               |
| [**DocVQA**](https://github.com/anisha2102/docvqa)                                  |                                             [DocVQA: A Dataset for VQA on Document Images](https://arxiv.org/abs/2312.15915)                                              |                                          Consists of 50,000 questions defined on 12,000+ document images                                           |
| [**InfoVQA**](https://www.docvqa.org/datasets/infographicvqa)                       |                                                           [InfographicVQA](https://arxiv.org/pdf/2104.12756v2)                                                            |                               Comprises a diverse collection of infographics along with question-answer annotations.                               |
| [**WTQ**](https://github.com/ppasupat/WikiTableQuestions)                           |                                      [Compositional Semantic Parsing on Semi-Structured Tables](https://arxiv.org/pdf/1508.00305v1)                                       |                                             A dataset of 22,033 complex questions on Wikipedia tables.                                             |
| [**TableFact**](https://tabfact.github.io/)                                         |                                   [TabFact : A Large-scale Dataset for Table-based Fact Verification](https://arxiv.org/abs/1909.02164)                                   |               A large-scale dataset with 16k Wikipedia tables as the evidence for 118k human-annotated natural language statements.                |
| [**MM-Math**](https://github.com/kge-sun/MM-Math)                                   |                 [MM-MATH: Advancing Multimodal Math Evaluation with Process Evaluation and Fine-grained Classification](https://arxiv.org/abs/2404.05091)                 |                  Consists of 5,929 open-ended middle school math problems with visual contexts, with fine-grained classification.                  |
| [**MathCheck**](https://mathcheck.github.io/)                                       |                      [Is Your Model Really A Good Math Reasoner? Evaluating Mathematical Reasoning with Checklist](https://arxiv.org/abs/2407.08733)                      |                                A well-designed checklist for testing task generalization and reasoning robustness.                                 |
| [**PuzzleVQA**](https://puzzlevqa.github.io/)                                       |                [PUZZLEVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns](https://arxiv.org/abs/2403.13315)                 |                                         A collection of 2000 puzzle instances based on abstract patterns.                                          |
| [**SMART-101**](http://smartdataset.github.io/smart101)                             |                                        [Are Deep Neural Networks SMARTer than Second Graders?](hhttps://arxiv.org/abs/2212.09993)                                         |            Evaluating the abstraction, deduction, and generalization abilities of neural networks in solving visul-linguistic puzzles.             |
| [**AlgpPuzzleVQA**](https://algopuzzlevqa.github.io/)                               |              [ARE LANGUAGE MODELS PUZZLE PRODIGIES? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning](https://arxiv.org/abs/2403.03864)              |                                             Evaluate the capabilities in solving algorithmic puzzles.                                              |
| [**ChartMimic**](https://github.com/ChartMimic/ChartMimic)                          |                      [ChartMimic: Evaluating LMM‚Äôs Cross-Modal Reasoning Capability via Chart-to-Code Generation](https://arxiv.org/abs/2406.09961)                       |                                      Aimed at assessing the visually- grounded code generation capabilities.                                       |
| [**ChartSumm**](https://github.com/pranonrahman/ChartSumm)                          |                  [ChartSumm: A Comprehensive Benchmark for Automatic Chart Summarization of Long and Short Summaries](https://arxiv.org/abs/2406.09961)                   |                                                                         -                                                                          |
| [**MMCode**](https://github.com/happylkx/MMCode)                                    |                  [MMCode: Evaluating Multi-Modal Code Large Language Models with Visually Rich Programming Problems](https://arxiv.org/pdf/2404.09486v1)                  |      Contains 3,548 questions and 6,620 images collected from real-world programming challenges harvested from 10 code competition websites.       |
| [**Design2Code**](https://salt-nlp.github.io/Design2Code/)                          |                                   [Design2Code: How Far Are We From Automating Front-End Engineering](https://arxiv.org/abs/2403.03163)                                   |                                           Manually curate a benchmark of 484 diverse real-world webpages                                           |
| [**Plot2Code**](https://huggingface.co/datasets/TencentARC/Plot2Code)               |    [Plot2Code: A Comprehensive Benchmark for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots](https://arxiv.org/abs/2405.07990)     |                                                      A comprehensive visual coding benchmark.                                                      |
| [**CharXiv**](https://charxiv.github.io/)                                           |                              [CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs](https://arxiv.org/abs/2406.18521)                               |                   A comprehensive evaluation suite in- volving 2,323 natural, challenging, and diverse charts from arXiv papers.                   |
| [**We-Math**](https://We-Math.github.io)                                            |                         [WE-MATH: Does Your Large Multimodal Model Achieve Human-like Mathematical Reasoning?](https://arxiv.org/abs/2407.01284)                          |                   6.5K visual math problems, spanning 67 hierarchical knowledge concepts and 5 layers of knowledge granularity.                    |
| [**SceMQA**](https://scemqa.github.io/)                                             |                          [SceMQA: A Scientific College Entrance Level Multimodal Question Answering Benchmark](https://arxiv.org/abs/2402.05138)                          |                               A benchmark for scientific multimodal question answering at the college entrance leve.                               |
| [**TheoremQA**](https://scemqa.github.io/)                                          |                                        [TheoremQA: A Theorem-driven Question Answering dataset](https://arxiv.org/abs/2305.12524)                                         |           Curated by domain experts containing 800 high-quality questions covering 350 theorems from Math, Physics, EE&CS, and Finance.            |

## Contributors

<a href="https://github.com/InfiMM/Awesome-Multimodal-LLM-for-Math-STEM/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=InfiMM/Awesome-Multimodal-LLM-for-Math-STEM" />
</a>

---

If you have any question about this opinionated list, do not hesitate to create an issue.
